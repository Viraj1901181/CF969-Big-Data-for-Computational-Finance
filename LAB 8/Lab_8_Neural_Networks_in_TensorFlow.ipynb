{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab 8: Neural Networks in TensorFlow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPiZipMQk4B83dZIr5Pf8o3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Viraj1901181/CF969-Big-Data-for-Computational-Finance/blob/master/LAB%208/Lab_8_Neural_Networks_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFMCEKynKC9C",
        "colab_type": "text"
      },
      "source": [
        "#CF969 - Big Data for Computational Finance\n",
        "\n",
        "##Lab 8: Neural Networks in TensorFlow\n",
        "\n",
        "In the previous lab you have learned some of the basics of TensorFlow, and in particular you have worked with autodiff and gradient descent.\n",
        "\n",
        "In this week's lab you will see how to use those tools for building and training some basic neural networks in TensorFlow\n",
        "\n",
        "The present Notebook for this lab is based on a previous version written by Bart de Keijzer, and also on a tutorial and a big set of slides created by Dr. Michael Fairbank: he is the creator of most of the material that you see here. Some of the Python code scripts here are based on code from tensorflow.org and from https://github.com/aymericdamien/TensorFlow-Examples/, by Aymeric Damien.\n",
        "\n",
        "When going through these notes, you should work, as usual, to work on a new Jupyter Notebook where you recreate and re-execute all the code snippets in the present notebook. Please experiment with the pieces of code, take your time with them, and look up the meaning of the various statements in the online TensorFlow documentation whenever you do not fully understand what is happening.\n",
        "\n",
        "##A Neural Network in Only a Few Lines of Code\n",
        "\n",
        "TensorFlow has some of the basic and common neuron activation functions built-in. A tensor can be passed to such a function, after which said function is applied to each element of the tensor. In particular, there is the function sigmoid(A) which applies the sigmoid function to each element of tensor A.\n",
        "\n",
        "While in our theoretical treatment of neural networks, we usually represent a network's input by a column vector, in TensorFlow we will use a row vector to represent the input. We do this for ease of programming. Recall that for a sigmoid neuron, the sigmoid function is applied on a value that is a weighted sum of all inputs, plus a bias. Therefore, if we now represent the weights by a column vector w and the bias by a scalar b (i.e. rank 0 tensor), we can now write the application of a sigmoid neuron to the input vector x using the command tf.sigmoid(tf.matmul(x,w)+b) . Note however that for matmul to work, we must explicitly define x and w as rank 2 tensors, despite that they can be more naturally seen as rank 1 tensors. This is simply because matmul expects its argument to be of rank 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z68pCotKPit",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        },
        "outputId": "df8cf446-cd59-4cb0-f970-5148a9aa16f7"
      },
      "source": [
        "import tensorflow as tf\n",
        "sess = tf.Session()\n",
        "x=tf.constant([[1,1]], tf.float32) # our input vector\n",
        "w=tf.constant([[2],[1]],tf.float32) # weight vector of a sigmoid neuron\n",
        "b=tf.constant(1,tf.float32) # bias of a sigmoid neuron\n",
        "y=tf.sigmoid(tf.matmul(x,w) + b)\n",
        "print(sess.run(y))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[0.98201376]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3d7uy6JKX6c",
        "colab_type": "text"
      },
      "source": [
        "We can generalise this easily to applying the input to multiple sigmoid neurons (that is, a layer of neurons). Let A be a matrix where the columns are the weight vectors of all the neurons. Let b be a row vector of all the biases of the neurons. The command tf.sigmoid(tf.matmul(x,A)+b) then results in a row vector containing the output of each of the neurons.\n",
        "\n",
        "Exercise: Take the above code and modify it by adding a second neuron with weights 1 and -1, and with a bias of -0.5. The answer should be a row vector with the values 0.98201376 and 0.37754068."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LRpKwrFKXEw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "98bd0233-34fd-4585-f363-aaedaf50f940"
      },
      "source": [
        "import tensorflow as tf\n",
        "sess = tf.Session()\n",
        "x=tf.constant([[1,1]], tf.float32) # our input vector\n",
        "w=tf.constant([[1],[-1]],tf.float32) # weight vector of a sigmoid neuron\n",
        "b=tf.constant(0.5,tf.float32) # bias of a sigmoid neuron\n",
        "y=tf.sigmoid(tf.matmul(x,w) + b)\n",
        "print(sess.run(y))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.62245935]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6lGdsH5J_t4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}